{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(data_path):\n",
    "    class_labels = os.listdir(data_path) # reads directory names as class-labels\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for class_ in class_labels:\n",
    "        if class_ == '.DS_Store':\n",
    "            continue\n",
    "        class_path = data_path+'/'+class_\n",
    "        imgs = os.listdir(class_path) # reads images names to read\n",
    "        for img in imgs:\n",
    "            if img == '.DS_Store':\n",
    "                continue\n",
    "            data.append(cv2.imread(class_path+'/'+img, cv2.IMREAD_GRAYSCALE))\n",
    "            labels.append(int(class_))\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of data\n",
      "No. of train images: 11385\n",
      "No. of test images: 3795\n",
      "No. of val images: 3795\n"
     ]
    }
   ],
   "source": [
    "test_path='./Group_10/test'\n",
    "train_path='./Group_10/train'\n",
    "val_path='./Group_10/val'\n",
    "# test_data, test_labels = data_import(test_path)\n",
    "# train_data, train_labels = data_import(train_path)\n",
    "# val_data, val_labels = data_import(val_path)\n",
    "\n",
    "# with open('test_data', mode='wb') as f:\n",
    "#     pickle.dump(test_data, f)\n",
    "# with open('train_data', mode='wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# with open('val_data', mode='wb') as f:\n",
    "#     pickle.dump(val_data, f)\n",
    "\n",
    "# with open('test_labels', mode='wb') as f:\n",
    "#     pickle.dump(test_labels, f)\n",
    "# with open('train_labels', mode='wb') as f:\n",
    "#     pickle.dump(train_labels, f)\n",
    "# with open('val_labels', mode='wb') as f:\n",
    "#     pickle.dump(val_labels, f)\n",
    "\n",
    "with open('test_data', mode='rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open('train_data', mode='rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('val_data', mode='rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "\n",
    "with open('test_labels', mode='rb') as f:\n",
    "    test_labels = pickle.load(f)\n",
    "with open('train_labels', mode='rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "with open('val_labels', mode='rb') as f:\n",
    "    val_labels = pickle.load(f)\n",
    "\n",
    "print('Summary of data')\n",
    "print(f'No. of train images: {len(train_data)}')\n",
    "print(f'No. of test images: {len(test_data)}')\n",
    "print(f'No. of val images: {len(val_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FCNN_3layer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Flatten)       (None, 784)               0         \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 250)               196250    \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 400)               100400    \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 100)               40100     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 337,760\n",
      "Trainable params: 337,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=6)\n",
    "# Three layer FCNN\n",
    "model_3 = keras.Sequential([\n",
    "    Flatten(input_shape=(28,28), name='Input_layer'), # image data as input\n",
    "    Dense(250, activation='sigmoid', name='Hidden_layer_1'),\n",
    "    Dense(400, activation='sigmoid', name='Hidden_layer_2'),\n",
    "    Dense(100, activation='sigmoid', name='Hidden_layer_3'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name='FCNN_3layer')\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4, patience=2, verbose=1)\n",
    "# modelCheckpointsSGD_momentum = keras.callbacks.ModelCheckpoint(filepath='./modelCheckpoints/SGD_momentum/model.{epoch:02d}-{loss:.2f}.h5', verbose=0)\n",
    "adagrad = keras.optimizers.Adagrad(learning_rate=0.001, epsilon=1e-10, name='AdaGrad')\n",
    "model_3.compile(optimizer=adagrad,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:39:54.366035: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11385/11385 [==============================] - 11s 991us/step - loss: 1.1684 - accuracy: 0.7849 - val_loss: 0.6948 - val_accuracy: 0.9304\n",
      "Epoch 2/100000\n",
      " 5104/11385 [============>.................] - ETA: 5s - loss: 0.5819 - accuracy: 0.9387"
     ]
    }
   ],
   "source": [
    "history = model_3.fit(x=train_data, y=train_labels, batch_size=1, epochs=100_000,\n",
    "                    callbacks=[earlystopping],\n",
    "                    verbose=1, shuffle=True,\n",
    "                    validation_split=0.0, validation_data=(val_data, val_labels), validation_batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save('AdaGrad-250,400,100.h5', overwrite=False, include_optimizer=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"AdaGrad\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model_3.predict(test_data, verbose=0)\n",
    "pred_labels = np.argmax(pred_labels, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(test_labels, pred_labels, num_classes=10)\n",
    "print('(AdaGrad)Confusion matrix on test data:\\n')\n",
    "print(confusion_matrix.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
