{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(data_path):\n",
    "    class_labels = os.listdir(data_path) # reads directory names as class-labels\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for class_ in class_labels:\n",
    "        if class_ == '.DS_Store':\n",
    "            continue\n",
    "        class_path = data_path+'/'+class_\n",
    "        imgs = os.listdir(class_path) # reads images names to read\n",
    "        for img in imgs:\n",
    "            if img == '.DS_Store':\n",
    "                continue\n",
    "            data.append(cv2.imread(class_path+'/'+img, cv2.IMREAD_GRAYSCALE))\n",
    "            labels.append(int(class_))\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of data\n",
      "No. of train images: 11385\n",
      "No. of test images: 3795\n",
      "No. of val images: 3795\n"
     ]
    }
   ],
   "source": [
    "test_path='./Group_10/test'\n",
    "train_path='./Group_10/train'\n",
    "val_path='./Group_10/val'\n",
    "# test_data, test_labels = data_import(test_path)\n",
    "# train_data, train_labels = data_import(train_path)\n",
    "# val_data, val_labels = data_import(val_path)\n",
    "\n",
    "# with open('test_data', mode='wb') as f:\n",
    "#     pickle.dump(test_data, f)\n",
    "# with open('train_data', mode='wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# with open('val_data', mode='wb') as f:\n",
    "#     pickle.dump(val_data, f)\n",
    "\n",
    "# with open('test_labels', mode='wb') as f:\n",
    "#     pickle.dump(test_labels, f)\n",
    "# with open('train_labels', mode='wb') as f:\n",
    "#     pickle.dump(train_labels, f)\n",
    "# with open('val_labels', mode='wb') as f:\n",
    "#     pickle.dump(val_labels, f)\n",
    "\n",
    "with open('test_data', mode='rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open('train_data', mode='rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('val_data', mode='rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "\n",
    "with open('test_labels', mode='rb') as f:\n",
    "    test_labels = pickle.load(f)\n",
    "with open('train_labels', mode='rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "with open('val_labels', mode='rb') as f:\n",
    "    val_labels = pickle.load(f)\n",
    "\n",
    "print('Summary of data')\n",
    "print(f'No. of train images: {len(train_data)}')\n",
    "print(f'No. of test images: {len(test_data)}')\n",
    "print(f'No. of val images: {len(val_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FCNN_3layer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Flatten)       (None, 784)               0         \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 250)               196250    \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 300)               75300     \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 100)               30100     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 302,660\n",
      "Trainable params: 302,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=6)\n",
    "# Three layer FCNN\n",
    "model_3 = keras.Sequential([\n",
    "    Flatten(input_shape=(28,28), name='Input_layer'), # image data as input\n",
    "    Dense(250, activation='sigmoid', kernel_initializer=initializer, bias_initializer=keras.initializers.Zeros(), name='Hidden_layer_1'),\n",
    "    Dense(300, activation='sigmoid', kernel_initializer=initializer, bias_initializer=keras.initializers.Zeros(), name='Hidden_layer_2'),\n",
    "    Dense(100, activation='sigmoid', kernel_initializer=initializer, bias_initializer=keras.initializers.Zeros(), name='Hidden_layer_3'),\n",
    "    Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=keras.initializers.Zeros(), name='Output')\n",
    "], name='FCNN_3layer')\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4, patience=3, verbose=1)\n",
    "# modelCheckpointsSGD_momentum = keras.callbacks.ModelCheckpoint(filepath='./modelCheckpoints/SGD_momentum/model.{epoch:02d}-{loss:.2f}.h5', verbose=0)\n",
    "sgd_moment_optimizer = keras.optimizers.Adagrad(learning_rate=0.001, epsilon=1e-10, name='AdaGrad')\n",
    "model_3.compile(optimizer=sgd_moment_optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 12:54:18.373001: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11385/11385 [==============================] - 14s 1ms/step - loss: 1.3930 - accuracy: 0.5311 - val_loss: 0.9066 - val_accuracy: 0.6659\n",
      "Epoch 2/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.7905 - accuracy: 0.7203 - val_loss: 0.7186 - val_accuracy: 0.7368\n",
      "Epoch 3/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.6491 - accuracy: 0.7792 - val_loss: 0.6232 - val_accuracy: 0.7747\n",
      "Epoch 4/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.5692 - accuracy: 0.8100 - val_loss: 0.5660 - val_accuracy: 0.8000\n",
      "Epoch 5/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.5141 - accuracy: 0.8328 - val_loss: 0.5200 - val_accuracy: 0.8203\n",
      "Epoch 6/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.4730 - accuracy: 0.8485 - val_loss: 0.4877 - val_accuracy: 0.8358\n",
      "Epoch 7/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.4416 - accuracy: 0.8585 - val_loss: 0.4621 - val_accuracy: 0.8435\n",
      "Epoch 8/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.4162 - accuracy: 0.8680 - val_loss: 0.4405 - val_accuracy: 0.8498\n",
      "Epoch 9/100000\n",
      "11385/11385 [==============================] - 13s 1ms/step - loss: 0.3952 - accuracy: 0.8746 - val_loss: 0.4226 - val_accuracy: 0.8567\n",
      "Epoch 10/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.3777 - accuracy: 0.8814 - val_loss: 0.4075 - val_accuracy: 0.8640\n",
      "Epoch 11/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.3626 - accuracy: 0.8855 - val_loss: 0.3936 - val_accuracy: 0.8704\n",
      "Epoch 12/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.3494 - accuracy: 0.8891 - val_loss: 0.3819 - val_accuracy: 0.8740\n",
      "Epoch 13/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.3377 - accuracy: 0.8926 - val_loss: 0.3712 - val_accuracy: 0.8780\n",
      "Epoch 14/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.3273 - accuracy: 0.8955 - val_loss: 0.3619 - val_accuracy: 0.8814\n",
      "Epoch 15/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.3178 - accuracy: 0.8997 - val_loss: 0.3539 - val_accuracy: 0.8833\n",
      "Epoch 16/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.3093 - accuracy: 0.9019 - val_loss: 0.3461 - val_accuracy: 0.8859\n",
      "Epoch 17/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.3015 - accuracy: 0.9047 - val_loss: 0.3387 - val_accuracy: 0.8877\n",
      "Epoch 18/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.2944 - accuracy: 0.9071 - val_loss: 0.3324 - val_accuracy: 0.8899\n",
      "Epoch 19/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.2877 - accuracy: 0.9086 - val_loss: 0.3265 - val_accuracy: 0.8904\n",
      "Epoch 20/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.2815 - accuracy: 0.9112 - val_loss: 0.3216 - val_accuracy: 0.8935\n",
      "Epoch 21/100000\n",
      "11385/11385 [==============================] - 10s 865us/step - loss: 0.2759 - accuracy: 0.9137 - val_loss: 0.3165 - val_accuracy: 0.8954\n",
      "Epoch 22/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.2706 - accuracy: 0.9148 - val_loss: 0.3113 - val_accuracy: 0.8954\n",
      "Epoch 23/100000\n",
      "11385/11385 [==============================] - 10s 873us/step - loss: 0.2656 - accuracy: 0.9172 - val_loss: 0.3071 - val_accuracy: 0.8986\n",
      "Epoch 24/100000\n",
      "11385/11385 [==============================] - 10s 866us/step - loss: 0.2610 - accuracy: 0.9185 - val_loss: 0.3032 - val_accuracy: 0.8996\n",
      "Epoch 25/100000\n",
      "11385/11385 [==============================] - 10s 871us/step - loss: 0.2565 - accuracy: 0.9193 - val_loss: 0.2989 - val_accuracy: 0.9004\n",
      "Epoch 26/100000\n",
      "11385/11385 [==============================] - 10s 875us/step - loss: 0.2524 - accuracy: 0.9207 - val_loss: 0.2956 - val_accuracy: 0.9017\n",
      "Epoch 27/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.2485 - accuracy: 0.9225 - val_loss: 0.2922 - val_accuracy: 0.9036\n",
      "Epoch 28/100000\n",
      "11385/11385 [==============================] - 10s 884us/step - loss: 0.2448 - accuracy: 0.9237 - val_loss: 0.2888 - val_accuracy: 0.9033\n",
      "Epoch 29/100000\n",
      "11385/11385 [==============================] - 11s 945us/step - loss: 0.2413 - accuracy: 0.9247 - val_loss: 0.2856 - val_accuracy: 0.9043\n",
      "Epoch 30/100000\n",
      "11385/11385 [==============================] - 11s 934us/step - loss: 0.2380 - accuracy: 0.9264 - val_loss: 0.2827 - val_accuracy: 0.9054\n",
      "Epoch 31/100000\n",
      "11385/11385 [==============================] - 10s 877us/step - loss: 0.2348 - accuracy: 0.9278 - val_loss: 0.2800 - val_accuracy: 0.9065\n",
      "Epoch 32/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.2317 - accuracy: 0.9286 - val_loss: 0.2774 - val_accuracy: 0.9067\n",
      "Epoch 33/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.2287 - accuracy: 0.9302 - val_loss: 0.2748 - val_accuracy: 0.9080\n",
      "Epoch 34/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.2258 - accuracy: 0.9303 - val_loss: 0.2726 - val_accuracy: 0.9094\n",
      "Epoch 35/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.2231 - accuracy: 0.9318 - val_loss: 0.2702 - val_accuracy: 0.9091\n",
      "Epoch 36/100000\n",
      "11385/11385 [==============================] - 10s 870us/step - loss: 0.2205 - accuracy: 0.9323 - val_loss: 0.2682 - val_accuracy: 0.9096\n",
      "Epoch 37/100000\n",
      "11385/11385 [==============================] - 10s 871us/step - loss: 0.2181 - accuracy: 0.9332 - val_loss: 0.2659 - val_accuracy: 0.9094\n",
      "Epoch 38/100000\n",
      "11385/11385 [==============================] - 10s 876us/step - loss: 0.2157 - accuracy: 0.9334 - val_loss: 0.2639 - val_accuracy: 0.9099\n",
      "Epoch 39/100000\n",
      "11385/11385 [==============================] - 10s 866us/step - loss: 0.2135 - accuracy: 0.9346 - val_loss: 0.2619 - val_accuracy: 0.9101\n",
      "Epoch 40/100000\n",
      "11385/11385 [==============================] - 10s 873us/step - loss: 0.2112 - accuracy: 0.9360 - val_loss: 0.2601 - val_accuracy: 0.9107\n",
      "Epoch 41/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.2091 - accuracy: 0.9362 - val_loss: 0.2582 - val_accuracy: 0.9123\n",
      "Epoch 42/100000\n",
      "11385/11385 [==============================] - 10s 873us/step - loss: 0.2071 - accuracy: 0.9371 - val_loss: 0.2566 - val_accuracy: 0.9123\n",
      "Epoch 43/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.2051 - accuracy: 0.9376 - val_loss: 0.2548 - val_accuracy: 0.9133\n",
      "Epoch 44/100000\n",
      "11385/11385 [==============================] - 10s 879us/step - loss: 0.2031 - accuracy: 0.9380 - val_loss: 0.2529 - val_accuracy: 0.9141\n",
      "Epoch 45/100000\n",
      "11385/11385 [==============================] - 10s 878us/step - loss: 0.2012 - accuracy: 0.9383 - val_loss: 0.2514 - val_accuracy: 0.9144\n",
      "Epoch 46/100000\n",
      "11385/11385 [==============================] - 10s 870us/step - loss: 0.1993 - accuracy: 0.9392 - val_loss: 0.2498 - val_accuracy: 0.9144\n",
      "Epoch 47/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.1974 - accuracy: 0.9391 - val_loss: 0.2483 - val_accuracy: 0.9157\n",
      "Epoch 48/100000\n",
      "11385/11385 [==============================] - 10s 876us/step - loss: 0.1958 - accuracy: 0.9407 - val_loss: 0.2467 - val_accuracy: 0.9159\n",
      "Epoch 49/100000\n",
      "11385/11385 [==============================] - 10s 880us/step - loss: 0.1941 - accuracy: 0.9411 - val_loss: 0.2456 - val_accuracy: 0.9162\n",
      "Epoch 50/100000\n",
      "11385/11385 [==============================] - 10s 865us/step - loss: 0.1925 - accuracy: 0.9417 - val_loss: 0.2440 - val_accuracy: 0.9167\n",
      "Epoch 51/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.1910 - accuracy: 0.9417 - val_loss: 0.2429 - val_accuracy: 0.9170\n",
      "Epoch 52/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.1893 - accuracy: 0.9425 - val_loss: 0.2413 - val_accuracy: 0.9178\n",
      "Epoch 53/100000\n",
      "11385/11385 [==============================] - 10s 864us/step - loss: 0.1878 - accuracy: 0.9426 - val_loss: 0.2401 - val_accuracy: 0.9186\n",
      "Epoch 54/100000\n",
      "11385/11385 [==============================] - 10s 873us/step - loss: 0.1864 - accuracy: 0.9433 - val_loss: 0.2391 - val_accuracy: 0.9183\n",
      "Epoch 55/100000\n",
      "11385/11385 [==============================] - 10s 871us/step - loss: 0.1850 - accuracy: 0.9435 - val_loss: 0.2379 - val_accuracy: 0.9186\n",
      "Epoch 56/100000\n",
      "11385/11385 [==============================] - 10s 870us/step - loss: 0.1837 - accuracy: 0.9440 - val_loss: 0.2368 - val_accuracy: 0.9191\n",
      "Epoch 57/100000\n",
      "11385/11385 [==============================] - 10s 877us/step - loss: 0.1825 - accuracy: 0.9445 - val_loss: 0.2355 - val_accuracy: 0.9207\n",
      "Epoch 58/100000\n",
      "11385/11385 [==============================] - 10s 881us/step - loss: 0.1812 - accuracy: 0.9450 - val_loss: 0.2344 - val_accuracy: 0.9217\n",
      "Epoch 59/100000\n",
      "11385/11385 [==============================] - 10s 876us/step - loss: 0.1799 - accuracy: 0.9448 - val_loss: 0.2335 - val_accuracy: 0.9217\n",
      "Epoch 60/100000\n",
      "11385/11385 [==============================] - 10s 868us/step - loss: 0.1787 - accuracy: 0.9458 - val_loss: 0.2325 - val_accuracy: 0.9225\n",
      "Epoch 61/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.1776 - accuracy: 0.9459 - val_loss: 0.2313 - val_accuracy: 0.9233\n",
      "Epoch 62/100000\n",
      "11385/11385 [==============================] - 11s 953us/step - loss: 0.1764 - accuracy: 0.9462 - val_loss: 0.2304 - val_accuracy: 0.9236\n",
      "Epoch 63/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1753 - accuracy: 0.9466 - val_loss: 0.2294 - val_accuracy: 0.9236\n",
      "Epoch 64/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1742 - accuracy: 0.9468 - val_loss: 0.2283 - val_accuracy: 0.9244\n",
      "Epoch 65/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1731 - accuracy: 0.9467 - val_loss: 0.2275 - val_accuracy: 0.9246\n",
      "Epoch 66/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1720 - accuracy: 0.9474 - val_loss: 0.2265 - val_accuracy: 0.9249\n",
      "Epoch 67/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1710 - accuracy: 0.9477 - val_loss: 0.2256 - val_accuracy: 0.9249\n",
      "Epoch 68/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1700 - accuracy: 0.9480 - val_loss: 0.2248 - val_accuracy: 0.9254\n",
      "Epoch 69/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1690 - accuracy: 0.9481 - val_loss: 0.2240 - val_accuracy: 0.9252\n",
      "Epoch 70/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1680 - accuracy: 0.9484 - val_loss: 0.2230 - val_accuracy: 0.9260\n",
      "Epoch 71/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1671 - accuracy: 0.9491 - val_loss: 0.2223 - val_accuracy: 0.9260\n",
      "Epoch 72/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1662 - accuracy: 0.9490 - val_loss: 0.2217 - val_accuracy: 0.9260\n",
      "Epoch 73/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1653 - accuracy: 0.9494 - val_loss: 0.2208 - val_accuracy: 0.9262\n",
      "Epoch 74/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1644 - accuracy: 0.9499 - val_loss: 0.2200 - val_accuracy: 0.9270\n",
      "Epoch 75/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1635 - accuracy: 0.9505 - val_loss: 0.2193 - val_accuracy: 0.9273\n",
      "Epoch 76/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1626 - accuracy: 0.9505 - val_loss: 0.2186 - val_accuracy: 0.9273\n",
      "Epoch 77/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1617 - accuracy: 0.9512 - val_loss: 0.2178 - val_accuracy: 0.9278\n",
      "Epoch 78/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1608 - accuracy: 0.9515 - val_loss: 0.2172 - val_accuracy: 0.9275\n",
      "Epoch 79/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1600 - accuracy: 0.9516 - val_loss: 0.2165 - val_accuracy: 0.9283\n",
      "Epoch 80/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1592 - accuracy: 0.9522 - val_loss: 0.2156 - val_accuracy: 0.9286\n",
      "Epoch 81/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1583 - accuracy: 0.9522 - val_loss: 0.2150 - val_accuracy: 0.9286\n",
      "Epoch 82/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1575 - accuracy: 0.9528 - val_loss: 0.2144 - val_accuracy: 0.9289\n",
      "Epoch 83/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1567 - accuracy: 0.9526 - val_loss: 0.2137 - val_accuracy: 0.9289\n",
      "Epoch 84/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1559 - accuracy: 0.9534 - val_loss: 0.2131 - val_accuracy: 0.9289\n",
      "Epoch 85/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1551 - accuracy: 0.9530 - val_loss: 0.2125 - val_accuracy: 0.9289\n",
      "Epoch 86/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1544 - accuracy: 0.9535 - val_loss: 0.2120 - val_accuracy: 0.9294\n",
      "Epoch 87/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1537 - accuracy: 0.9538 - val_loss: 0.2113 - val_accuracy: 0.9296\n",
      "Epoch 88/100000\n",
      "11385/11385 [==============================] - 794s 70ms/step - loss: 0.1530 - accuracy: 0.9540 - val_loss: 0.2108 - val_accuracy: 0.9296\n",
      "Epoch 89/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1523 - accuracy: 0.9546 - val_loss: 0.2101 - val_accuracy: 0.9296\n",
      "Epoch 90/100000\n",
      "11385/11385 [==============================] - 10s 902us/step - loss: 0.1516 - accuracy: 0.9548 - val_loss: 0.2096 - val_accuracy: 0.9296\n",
      "Epoch 91/100000\n",
      "11385/11385 [==============================] - 10s 877us/step - loss: 0.1509 - accuracy: 0.9549 - val_loss: 0.2091 - val_accuracy: 0.9294\n",
      "Epoch 92/100000\n",
      "11385/11385 [==============================] - 10s 909us/step - loss: 0.1503 - accuracy: 0.9548 - val_loss: 0.2086 - val_accuracy: 0.9296\n",
      "Epoch 93/100000\n",
      "11385/11385 [==============================] - 11s 932us/step - loss: 0.1496 - accuracy: 0.9553 - val_loss: 0.2081 - val_accuracy: 0.9294\n",
      "Epoch 94/100000\n",
      "11385/11385 [==============================] - 11s 953us/step - loss: 0.1490 - accuracy: 0.9559 - val_loss: 0.2076 - val_accuracy: 0.9299\n",
      "Epoch 95/100000\n",
      "11385/11385 [==============================] - 10s 914us/step - loss: 0.1483 - accuracy: 0.9556 - val_loss: 0.2070 - val_accuracy: 0.9304\n",
      "Epoch 96/100000\n",
      "11385/11385 [==============================] - 10s 887us/step - loss: 0.1476 - accuracy: 0.9563 - val_loss: 0.2065 - val_accuracy: 0.9310\n",
      "Epoch 97/100000\n",
      "11385/11385 [==============================] - 10s 880us/step - loss: 0.1470 - accuracy: 0.9569 - val_loss: 0.2059 - val_accuracy: 0.9315\n",
      "Epoch 98/100000\n",
      "11385/11385 [==============================] - 10s 877us/step - loss: 0.1464 - accuracy: 0.9570 - val_loss: 0.2054 - val_accuracy: 0.9318\n",
      "Epoch 99/100000\n",
      "11385/11385 [==============================] - 10s 879us/step - loss: 0.1458 - accuracy: 0.9574 - val_loss: 0.2048 - val_accuracy: 0.9315\n",
      "Epoch 100/100000\n",
      "11385/11385 [==============================] - 10s 900us/step - loss: 0.1452 - accuracy: 0.9574 - val_loss: 0.2043 - val_accuracy: 0.9315\n",
      "Epoch 101/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1446 - accuracy: 0.9578 - val_loss: 0.2039 - val_accuracy: 0.9315\n",
      "Epoch 102/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1440 - accuracy: 0.9578 - val_loss: 0.2034 - val_accuracy: 0.9315\n",
      "Epoch 103/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1435 - accuracy: 0.9583 - val_loss: 0.2029 - val_accuracy: 0.9320\n",
      "Epoch 104/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1429 - accuracy: 0.9583 - val_loss: 0.2024 - val_accuracy: 0.9320\n",
      "Epoch 105/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.1424 - accuracy: 0.9585 - val_loss: 0.2019 - val_accuracy: 0.9320\n",
      "Epoch 106/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.1418 - accuracy: 0.9585 - val_loss: 0.2015 - val_accuracy: 0.9320\n",
      "Epoch 107/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1412 - accuracy: 0.9585 - val_loss: 0.2010 - val_accuracy: 0.9323\n",
      "Epoch 108/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.1407 - accuracy: 0.9587 - val_loss: 0.2005 - val_accuracy: 0.9325\n",
      "Epoch 109/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.1402 - accuracy: 0.9588 - val_loss: 0.2000 - val_accuracy: 0.9328\n",
      "Epoch 110/100000\n",
      "11385/11385 [==============================] - 11s 930us/step - loss: 0.1396 - accuracy: 0.9588 - val_loss: 0.1994 - val_accuracy: 0.9331\n",
      "Epoch 111/100000\n",
      "11385/11385 [==============================] - 10s 874us/step - loss: 0.1391 - accuracy: 0.9592 - val_loss: 0.1990 - val_accuracy: 0.9331\n",
      "Epoch 112/100000\n",
      "11385/11385 [==============================] - 10s 869us/step - loss: 0.1385 - accuracy: 0.9593 - val_loss: 0.1986 - val_accuracy: 0.9336\n",
      "Epoch 113/100000\n",
      "11385/11385 [==============================] - 10s 884us/step - loss: 0.1380 - accuracy: 0.9597 - val_loss: 0.1982 - val_accuracy: 0.9341\n",
      "Epoch 114/100000\n",
      "11385/11385 [==============================] - 10s 903us/step - loss: 0.1375 - accuracy: 0.9597 - val_loss: 0.1978 - val_accuracy: 0.9341\n",
      "Epoch 115/100000\n",
      "11385/11385 [==============================] - 11s 979us/step - loss: 0.1370 - accuracy: 0.9597 - val_loss: 0.1974 - val_accuracy: 0.9341\n",
      "Epoch 116/100000\n",
      "11385/11385 [==============================] - 11s 996us/step - loss: 0.1366 - accuracy: 0.9600 - val_loss: 0.1970 - val_accuracy: 0.9341\n",
      "Epoch 117/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1361 - accuracy: 0.9599 - val_loss: 0.1966 - val_accuracy: 0.9341\n",
      "Epoch 118/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1356 - accuracy: 0.9607 - val_loss: 0.1963 - val_accuracy: 0.9341\n",
      "Epoch 119/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1352 - accuracy: 0.9606 - val_loss: 0.1958 - val_accuracy: 0.9341\n",
      "Epoch 120/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1347 - accuracy: 0.9607 - val_loss: 0.1954 - val_accuracy: 0.9344\n",
      "Epoch 121/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1342 - accuracy: 0.9610 - val_loss: 0.1951 - val_accuracy: 0.9352\n",
      "Epoch 122/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1337 - accuracy: 0.9615 - val_loss: 0.1948 - val_accuracy: 0.9352\n",
      "Epoch 123/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1333 - accuracy: 0.9616 - val_loss: 0.1943 - val_accuracy: 0.9352\n",
      "Epoch 124/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1328 - accuracy: 0.9619 - val_loss: 0.1939 - val_accuracy: 0.9352\n",
      "Epoch 125/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1324 - accuracy: 0.9623 - val_loss: 0.1936 - val_accuracy: 0.9352\n",
      "Epoch 126/100000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.1319 - accuracy: 0.9621 - val_loss: 0.1932 - val_accuracy: 0.9354\n",
      "Epoch 127/100000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.1315 - accuracy: 0.9625 - val_loss: 0.1929 - val_accuracy: 0.9354\n",
      "Epoch 128/100000\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.1311 - accuracy: 0.9628 - val_loss: 0.1925 - val_accuracy: 0.9354\n",
      "Epoch 129/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1307 - accuracy: 0.9626 - val_loss: 0.1923 - val_accuracy: 0.9354\n",
      "Epoch 130/100000\n",
      "11385/11385 [==============================] - 10s 887us/step - loss: 0.1303 - accuracy: 0.9628 - val_loss: 0.1919 - val_accuracy: 0.9354\n",
      "Epoch 131/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1298 - accuracy: 0.9630 - val_loss: 0.1916 - val_accuracy: 0.9354\n",
      "Epoch 132/100000\n",
      "11385/11385 [==============================] - 10s 887us/step - loss: 0.1294 - accuracy: 0.9632 - val_loss: 0.1912 - val_accuracy: 0.9354\n",
      "Epoch 133/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1290 - accuracy: 0.9633 - val_loss: 0.1909 - val_accuracy: 0.9354\n",
      "Epoch 134/100000\n",
      "11385/11385 [==============================] - 10s 888us/step - loss: 0.1286 - accuracy: 0.9634 - val_loss: 0.1906 - val_accuracy: 0.9354\n",
      "Epoch 135/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1282 - accuracy: 0.9634 - val_loss: 0.1904 - val_accuracy: 0.9354\n",
      "Epoch 136/100000\n",
      "11385/11385 [==============================] - 10s 895us/step - loss: 0.1278 - accuracy: 0.9635 - val_loss: 0.1901 - val_accuracy: 0.9354\n",
      "Epoch 137/100000\n",
      "11385/11385 [==============================] - 10s 888us/step - loss: 0.1274 - accuracy: 0.9639 - val_loss: 0.1898 - val_accuracy: 0.9352\n",
      "Epoch 138/100000\n",
      "11385/11385 [==============================] - 10s 886us/step - loss: 0.1270 - accuracy: 0.9635 - val_loss: 0.1895 - val_accuracy: 0.9357\n",
      "Epoch 139/100000\n",
      "11385/11385 [==============================] - 10s 882us/step - loss: 0.1266 - accuracy: 0.9641 - val_loss: 0.1892 - val_accuracy: 0.9360\n",
      "Epoch 140/100000\n",
      "11385/11385 [==============================] - 10s 915us/step - loss: 0.1262 - accuracy: 0.9639 - val_loss: 0.1889 - val_accuracy: 0.9362\n",
      "Epoch 141/100000\n",
      "11385/11385 [==============================] - 10s 907us/step - loss: 0.1259 - accuracy: 0.9641 - val_loss: 0.1885 - val_accuracy: 0.9362\n",
      "Epoch 142/100000\n",
      "11385/11385 [==============================] - 10s 895us/step - loss: 0.1255 - accuracy: 0.9642 - val_loss: 0.1882 - val_accuracy: 0.9362\n",
      "Epoch 143/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1251 - accuracy: 0.9644 - val_loss: 0.1880 - val_accuracy: 0.9365\n",
      "Epoch 144/100000\n",
      "11385/11385 [==============================] - 10s 911us/step - loss: 0.1247 - accuracy: 0.9642 - val_loss: 0.1876 - val_accuracy: 0.9368\n",
      "Epoch 145/100000\n",
      "11385/11385 [==============================] - 10s 909us/step - loss: 0.1244 - accuracy: 0.9646 - val_loss: 0.1874 - val_accuracy: 0.9368\n",
      "Epoch 146/100000\n",
      "11385/11385 [==============================] - 10s 908us/step - loss: 0.1240 - accuracy: 0.9647 - val_loss: 0.1871 - val_accuracy: 0.9368\n",
      "Epoch 147/100000\n",
      "11385/11385 [==============================] - 10s 915us/step - loss: 0.1237 - accuracy: 0.9649 - val_loss: 0.1868 - val_accuracy: 0.9373\n",
      "Epoch 148/100000\n",
      "11385/11385 [==============================] - 10s 912us/step - loss: 0.1233 - accuracy: 0.9650 - val_loss: 0.1866 - val_accuracy: 0.9373\n",
      "Epoch 149/100000\n",
      "11385/11385 [==============================] - 11s 967us/step - loss: 0.1230 - accuracy: 0.9653 - val_loss: 0.1862 - val_accuracy: 0.9373\n",
      "Epoch 150/100000\n",
      "11385/11385 [==============================] - 11s 923us/step - loss: 0.1226 - accuracy: 0.9653 - val_loss: 0.1860 - val_accuracy: 0.9373\n",
      "Epoch 151/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1223 - accuracy: 0.9654 - val_loss: 0.1857 - val_accuracy: 0.9373\n",
      "Epoch 152/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1855 - val_accuracy: 0.9373\n",
      "Epoch 153/100000\n",
      "11385/11385 [==============================] - 10s 891us/step - loss: 0.1215 - accuracy: 0.9658 - val_loss: 0.1852 - val_accuracy: 0.9378\n",
      "Epoch 154/100000\n",
      "11385/11385 [==============================] - 10s 895us/step - loss: 0.1211 - accuracy: 0.9657 - val_loss: 0.1849 - val_accuracy: 0.9381\n",
      "Epoch 155/100000\n",
      "11385/11385 [==============================] - 10s 895us/step - loss: 0.1208 - accuracy: 0.9657 - val_loss: 0.1847 - val_accuracy: 0.9378\n",
      "Epoch 156/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1204 - accuracy: 0.9657 - val_loss: 0.1844 - val_accuracy: 0.9381\n",
      "Epoch 157/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1201 - accuracy: 0.9659 - val_loss: 0.1842 - val_accuracy: 0.9383\n",
      "Epoch 158/100000\n",
      "11385/11385 [==============================] - 10s 893us/step - loss: 0.1198 - accuracy: 0.9659 - val_loss: 0.1838 - val_accuracy: 0.9381\n",
      "Epoch 159/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1194 - accuracy: 0.9657 - val_loss: 0.1836 - val_accuracy: 0.9383\n",
      "Epoch 160/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1191 - accuracy: 0.9663 - val_loss: 0.1834 - val_accuracy: 0.9381\n",
      "Epoch 161/100000\n",
      "11385/11385 [==============================] - 10s 893us/step - loss: 0.1188 - accuracy: 0.9664 - val_loss: 0.1831 - val_accuracy: 0.9381\n",
      "Epoch 162/100000\n",
      "11385/11385 [==============================] - 10s 891us/step - loss: 0.1185 - accuracy: 0.9662 - val_loss: 0.1829 - val_accuracy: 0.9383\n",
      "Epoch 163/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1181 - accuracy: 0.9664 - val_loss: 0.1826 - val_accuracy: 0.9386\n",
      "Epoch 164/100000\n",
      "11385/11385 [==============================] - 10s 891us/step - loss: 0.1178 - accuracy: 0.9666 - val_loss: 0.1824 - val_accuracy: 0.9386\n",
      "Epoch 165/100000\n",
      "11385/11385 [==============================] - 10s 886us/step - loss: 0.1175 - accuracy: 0.9668 - val_loss: 0.1822 - val_accuracy: 0.9386\n",
      "Epoch 166/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1172 - accuracy: 0.9667 - val_loss: 0.1819 - val_accuracy: 0.9389\n",
      "Epoch 167/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1169 - accuracy: 0.9667 - val_loss: 0.1816 - val_accuracy: 0.9391\n",
      "Epoch 168/100000\n",
      "11385/11385 [==============================] - 10s 885us/step - loss: 0.1165 - accuracy: 0.9670 - val_loss: 0.1814 - val_accuracy: 0.9391\n",
      "Epoch 169/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1163 - accuracy: 0.9671 - val_loss: 0.1811 - val_accuracy: 0.9391\n",
      "Epoch 170/100000\n",
      "11385/11385 [==============================] - 10s 894us/step - loss: 0.1160 - accuracy: 0.9669 - val_loss: 0.1809 - val_accuracy: 0.9394\n",
      "Epoch 171/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1157 - accuracy: 0.9672 - val_loss: 0.1806 - val_accuracy: 0.9391\n",
      "Epoch 172/100000\n",
      "11385/11385 [==============================] - 10s 888us/step - loss: 0.1154 - accuracy: 0.9676 - val_loss: 0.1804 - val_accuracy: 0.9394\n",
      "Epoch 173/100000\n",
      "11385/11385 [==============================] - 10s 893us/step - loss: 0.1150 - accuracy: 0.9674 - val_loss: 0.1802 - val_accuracy: 0.9391\n",
      "Epoch 174/100000\n",
      "11385/11385 [==============================] - 10s 887us/step - loss: 0.1147 - accuracy: 0.9674 - val_loss: 0.1800 - val_accuracy: 0.9391\n",
      "Epoch 175/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1144 - accuracy: 0.9678 - val_loss: 0.1798 - val_accuracy: 0.9391\n",
      "Epoch 176/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1142 - accuracy: 0.9679 - val_loss: 0.1796 - val_accuracy: 0.9397\n",
      "Epoch 177/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1139 - accuracy: 0.9679 - val_loss: 0.1794 - val_accuracy: 0.9391\n",
      "Epoch 178/100000\n",
      "11385/11385 [==============================] - 10s 888us/step - loss: 0.1136 - accuracy: 0.9683 - val_loss: 0.1791 - val_accuracy: 0.9394\n",
      "Epoch 179/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1133 - accuracy: 0.9679 - val_loss: 0.1789 - val_accuracy: 0.9397\n",
      "Epoch 180/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1130 - accuracy: 0.9682 - val_loss: 0.1787 - val_accuracy: 0.9397\n",
      "Epoch 181/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1128 - accuracy: 0.9686 - val_loss: 0.1785 - val_accuracy: 0.9397\n",
      "Epoch 182/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1125 - accuracy: 0.9684 - val_loss: 0.1784 - val_accuracy: 0.9404\n",
      "Epoch 183/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1122 - accuracy: 0.9687 - val_loss: 0.1782 - val_accuracy: 0.9402\n",
      "Epoch 184/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1119 - accuracy: 0.9686 - val_loss: 0.1780 - val_accuracy: 0.9402\n",
      "Epoch 185/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1117 - accuracy: 0.9688 - val_loss: 0.1777 - val_accuracy: 0.9402\n",
      "Epoch 186/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1114 - accuracy: 0.9686 - val_loss: 0.1775 - val_accuracy: 0.9407\n",
      "Epoch 187/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1111 - accuracy: 0.9688 - val_loss: 0.1773 - val_accuracy: 0.9404\n",
      "Epoch 188/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1109 - accuracy: 0.9689 - val_loss: 0.1771 - val_accuracy: 0.9404\n",
      "Epoch 189/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1106 - accuracy: 0.9688 - val_loss: 0.1770 - val_accuracy: 0.9404\n",
      "Epoch 190/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1104 - accuracy: 0.9691 - val_loss: 0.1767 - val_accuracy: 0.9407\n",
      "Epoch 191/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1101 - accuracy: 0.9692 - val_loss: 0.1765 - val_accuracy: 0.9407\n",
      "Epoch 192/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1098 - accuracy: 0.9693 - val_loss: 0.1763 - val_accuracy: 0.9410\n",
      "Epoch 193/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1095 - accuracy: 0.9692 - val_loss: 0.1761 - val_accuracy: 0.9410\n",
      "Epoch 194/100000\n",
      "11385/11385 [==============================] - 12s 1ms/step - loss: 0.1093 - accuracy: 0.9693 - val_loss: 0.1759 - val_accuracy: 0.9412\n",
      "Epoch 195/100000\n",
      "11385/11385 [==============================] - 10s 907us/step - loss: 0.1090 - accuracy: 0.9695 - val_loss: 0.1757 - val_accuracy: 0.9410\n",
      "Epoch 196/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1088 - accuracy: 0.9695 - val_loss: 0.1755 - val_accuracy: 0.9410\n",
      "Epoch 197/100000\n",
      "11385/11385 [==============================] - 10s 899us/step - loss: 0.1086 - accuracy: 0.9697 - val_loss: 0.1753 - val_accuracy: 0.9410\n",
      "Epoch 198/100000\n",
      "11385/11385 [==============================] - 10s 891us/step - loss: 0.1083 - accuracy: 0.9700 - val_loss: 0.1751 - val_accuracy: 0.9407\n",
      "Epoch 199/100000\n",
      "11385/11385 [==============================] - 10s 894us/step - loss: 0.1080 - accuracy: 0.9698 - val_loss: 0.1750 - val_accuracy: 0.9407\n",
      "Epoch 200/100000\n",
      "11385/11385 [==============================] - 10s 892us/step - loss: 0.1078 - accuracy: 0.9700 - val_loss: 0.1747 - val_accuracy: 0.9404\n",
      "Epoch 201/100000\n",
      "11385/11385 [==============================] - 10s 895us/step - loss: 0.1075 - accuracy: 0.9700 - val_loss: 0.1746 - val_accuracy: 0.9407\n",
      "Epoch 202/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1073 - accuracy: 0.9700 - val_loss: 0.1744 - val_accuracy: 0.9404\n",
      "Epoch 203/100000\n",
      "11385/11385 [==============================] - 10s 888us/step - loss: 0.1070 - accuracy: 0.9703 - val_loss: 0.1742 - val_accuracy: 0.9402\n",
      "Epoch 204/100000\n",
      "11385/11385 [==============================] - 10s 898us/step - loss: 0.1068 - accuracy: 0.9706 - val_loss: 0.1740 - val_accuracy: 0.9404\n",
      "Epoch 205/100000\n",
      "11385/11385 [==============================] - 10s 890us/step - loss: 0.1065 - accuracy: 0.9703 - val_loss: 0.1738 - val_accuracy: 0.9404\n",
      "Epoch 206/100000\n",
      "11385/11385 [==============================] - 10s 885us/step - loss: 0.1063 - accuracy: 0.9707 - val_loss: 0.1736 - val_accuracy: 0.9404\n",
      "Epoch 207/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1061 - accuracy: 0.9708 - val_loss: 0.1735 - val_accuracy: 0.9410\n",
      "Epoch 208/100000\n",
      "11385/11385 [==============================] - 151s 13ms/step - loss: 0.1058 - accuracy: 0.9707 - val_loss: 0.1733 - val_accuracy: 0.9410\n",
      "Epoch 209/100000\n",
      "11385/11385 [==============================] - 10s 886us/step - loss: 0.1056 - accuracy: 0.9708 - val_loss: 0.1731 - val_accuracy: 0.9410\n",
      "Epoch 210/100000\n",
      "11385/11385 [==============================] - 10s 922us/step - loss: 0.1054 - accuracy: 0.9708 - val_loss: 0.1729 - val_accuracy: 0.9410\n",
      "Epoch 211/100000\n",
      "11385/11385 [==============================] - 10s 889us/step - loss: 0.1052 - accuracy: 0.9708 - val_loss: 0.1727 - val_accuracy: 0.9410\n",
      "Epoch 212/100000\n",
      " 6599/11385 [================>.............] - ETA: 3s - loss: 0.1019 - accuracy: 0.9729"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model_3\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_data, y\u001b[39m=\u001b[39;49mtrain_labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m100_000\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[earlystopping],\n\u001b[1;32m      3\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m                     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(val_data, val_labels), validation_batch_size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:415\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    406\u001b[0m         outputs \u001b[39m=\u001b[39m functional_ops\u001b[39m.\u001b[39mpartitioned_call(\n\u001b[1;32m    407\u001b[0m             args\u001b[39m=\u001b[39margs,\n\u001b[1;32m    408\u001b[0m             f\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m             config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m    412\u001b[0m             executor_type\u001b[39m=\u001b[39mexecutor_type)\n\u001b[1;32m    414\u001b[0m \u001b[39mfor\u001b[39;00m i, func_graph_output \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph_outputs):\n\u001b[0;32m--> 415\u001b[0m   handle_data_util\u001b[39m.\u001b[39;49mcopy_handle_data(func_graph_output, outputs[i])\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n\u001b[1;32m    417\u001b[0m   \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/.env/lib/python3.10/site-packages/tensorflow/python/ops/handle_data_util.py:42\u001b[0m, in \u001b[0;36mcopy_handle_data\u001b[0;34m(source_t, target_t)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy_handle_data\u001b[39m(source_t, target_t):\n\u001b[1;32m     27\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Copies HandleData for variant and resource type tensors if available.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m  The CppShapeInferenceResult::HandleData proto contains information about the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m    target_t: The tensor to copy HandleData to.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m   \u001b[39mif\u001b[39;00m (target_t\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mresource \u001b[39mor\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m       target_t\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mvariant):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source_t, ops\u001b[39m.\u001b[39mEagerTensor):\n\u001b[1;32m     44\u001b[0m       handle_data \u001b[39m=\u001b[39m source_t\u001b[39m.\u001b[39m_handle_data  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model_3.fit(x=train_data, y=train_labels, batch_size=1, epochs=100_000,\n",
    "                    callbacks=[earlystopping],\n",
    "                    verbose=1, shuffle=True,\n",
    "                    validation_split=0.0, validation_data=(val_data, val_labels), validation_batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"AdaGrad\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model_3.predict(test_data, verbose=0)\n",
    "pred_labels = np.argmax(pred_labels, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(test_labels, pred_labels, num_classes=10)\n",
    "print('(AdaGrad)Confusion matrix on test data:\\n')\n",
    "print(confusion_matrix.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
